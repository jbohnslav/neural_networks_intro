{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Convolutional Neuron Network \n",
    "\n",
    "### Comp Club 4/17/19\n",
    "Shih-Yi Tseng\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "##  Parts of Convolutional Neural Networks (Notes from Harvard 2019 CS109b)\n",
    "\n",
    "There are common types of layers in a Convolutional Neural Network:\n",
    "\n",
    "- Convolutional Layers\n",
    "- Pooling Layers\n",
    "- Dropout Layers\n",
    "- Fully Connected Layers\n",
    "\n",
    "### a. Convolutional Layers\n",
    "\n",
    "Convolutional layers are comprised of **filters** and **feature maps**. The filters are essentially the **neurons** of the layer. They have the weights and produce the input for the next layer. The feature map is the output of one filter applied to the previous layer. \n",
    "\n",
    "The fundamental difference between a densely connected layer and a convolution layer is that dense layers learn global patterns in their input feature space (for example, for an image, patterns involving all pixels), whereas convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs called *receptive fields*. \n",
    "\n",
    "This key characteristic gives convnets two interesting properties:\n",
    "\n",
    "- The patterns they learn are **translation invariant**. After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location. This makes convnets data efficient when processing images (because the visual world is fundamentally translation invariant): they need fewer training samples to learn representations that have generalization power.\n",
    "\n",
    "- They can learn **spatial hierarchies of patterns**. A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical).\n",
    "\n",
    "Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height and width) as well as a depth axis (also called the channels axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, the depth is 1 (levels of gray). The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for filters. Filters encode specific aspects of the input data: at a high level, a single filter could encode the concept “presence of a face in the input,” for instance.\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    "- Size of the patches extracted from the inputs. These are typically  3×3 or  5×5 \n",
    "- The number of filters computed by the convolution. \n",
    "\n",
    "**Padding**: One of \"valid\", \"causal\" or \"same\" (case-insensitive).  \"valid\" means \"no padding\".  \"same\" results in padding the input such that the output has the same length as the original input.  \"causal\" results in causal (dilated) convolutions,\n",
    "\n",
    "In `keras` see [convolutional layers](https://keras.io/layers/convolutional/)\n",
    "\n",
    "**keras.layers.Conv2D**(filters, kernel_size, strides=(1, 1), padding='valid', activation=None, use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', data_format='channels_last', \n",
    "                    bias_initializer='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How are the values in feature maps calculated?\n",
    "\n",
    "![title](convolution-many-filters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Pooling Layers\n",
    "\n",
    "Pooling layers are also comprised of filters and feature maps. Let's say the pooling layer has a 2x2 receptive field and a stride of 2. This stride results in feature maps that are one half the size of the input feature maps. We can use a max() operation for each receptive field. \n",
    "\n",
    "In `keras` see [pooling layers](https://keras.io/layers/pooling/)\n",
    "\n",
    "**keras.layers.MaxPooling2D**(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "\n",
    "![Max Pool](MaxPool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Dropout Layers\n",
    "\n",
    "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "In `keras` see [Dropout layers](https://keras.io/layers/core/)\n",
    "\n",
    "keras.layers.Dropout(rate, seed=None)\n",
    "\n",
    "rate: float between 0 and 1. Fraction of the input units to drop.<br>\n",
    "seed: A Python integer to use as random seed.\n",
    "\n",
    "References\n",
    "\n",
    "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Fully Connected Layers\n",
    "\n",
    "A fully connected layer flattens the square feature map into a vector. Then we can use a sigmoid or softmax activation function to output probabilities of classes. \n",
    "\n",
    "In `keras` see [FC layers](https://keras.io/layers/core/)\n",
    "\n",
    "**keras.layers.Dense**(units, activation=None, use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros')\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neuron Network in Action: Source Classification\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "In this exercise, we will learn how to build a CNN to classify sources for 2-photon calcium imaging data. In 2-photon calcium imaging, we collect images of field of views through the microscope, and process those images (or movies) afterward to extract activity traces of ROIs, or what we call \"sources\", before doing further analysis. Open source softwares such as Suite2P (https://github.com/cortex-lab/Suite2P) or CNMF (https://github.com/flatironinstitute/CaImAn) are available for processing the images and extract sources, and at the end of this pipeline, we classify the sources into different categories and determine what we want to include in the downstream analysis.\n",
    "\n",
    "Here is an example of the collection of all sources in one imaging plane of L2/3 cortex: \n",
    "![source](source_images.png)\n",
    "\n",
    "Our task is to classify these sources into 4 categories using a CNN:\n",
    "\n",
    "- **Class 0: Cell bodies**\n",
    "\n",
    "- **Class 1: Small, vertical processes**\n",
    "\n",
    "- **Class 2: Complex, horizontal processes or out-of-focus cells**\n",
    "\n",
    "- **Class 3: Junk**\n",
    "\n",
    "You'll see some examples in the following section.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libaries and functions\n",
    "\n",
    "***\n",
    "\n",
    "First we import some libraries and functions that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(11)\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and visualize the sources\n",
    "***\n",
    "\n",
    "Run this cell to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sources.pkl','rb') as file:\n",
    "    sources = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains 16000 images of different sources (height = 25, width = 25, number of channels = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4000 examples for each class (arranged in class order). So we can recreate the class labels (response variables) for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 4000\n",
    "class_name = ['cell','vertical process','complex process', 'junk']\n",
    "labels = [n * np.ones((class_size,1)) for n in range(len(class_name))]\n",
    "labels = np.concatenate(labels, axis = 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also do some visualization and see how these images look like. Try to change the `n_source` and look at different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_source = 0\n",
    "plt.imshow(sources[n_source,:,:,0],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(class_id, n_samples = 16):\n",
    "    sample_id = class_id * class_size + np.random.choice(class_size, size = (n_samples,))\n",
    "    image_list = [sources[ii,:,:,0] for ii in sample_id]\n",
    "    image_stack = np.hstack(image_list)\n",
    "    plt.figure(figsize = (20,3))\n",
    "    plt.imshow(image_stack, cmap = 'gray', vmin=None, vmax=0.4)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(class_name[class_id])\n",
    "    \n",
    "for n_class in range(4):\n",
    "    plot_samples(n_class, n_samples = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training/validation/test samples\n",
    "***\n",
    "As a standard practice for building machine learning models, we split our data into 3 portions: training, validation, and test sets using **train_test_split** funciton (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn. It also shuffle the data for you, conveniently.\n",
    "\n",
    "Here we use 80% of samples for training, 10% for validation and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80-20 for train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(sources, labels, test_size = 0.2)\n",
    "\n",
    "# further split the 20% in half for validation and true test set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to convert the labels into dummy variables (one column for each class) using keras utility function **to_categorical**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to categorical\n",
    "n_classes = 4\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation using ImageDataGenerator\n",
    "***\n",
    "\n",
    "For training CNN on image classification, one commonly used method to prevent overfitting is to do image augmentation. We can manipulate the training images in several ways to create a larger training set, such as random rotations, shifts, zooms, etc. Here we are using the **ImageDataGenerator** from Keras for the augmentation.\n",
    "\n",
    "You can find more options and description for the ImageDataGenerator from the documentation: https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 360,\n",
    "    width_shift_range = 0.15,\n",
    "    height_shift_range = 0.15,\n",
    "    zoom_range = 0.15,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True)\n",
    "\n",
    "# train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN model\n",
    "***\n",
    "We use Keras **Sequential** mode to build our model, in which we can stack layers on top of each other. And then we choose the **optimizer** we want to use, and **compile the model** with the optimizer, the loss function, and the metrics for evaluation. We can print out the summary of the model after compiling it.\n",
    "\n",
    "\n",
    "Documentation for Sequential: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = X_train.shape[1:], activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "# set optimizer\n",
    "opt = keras.optimizers.Adam(lr = 0.01)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss ='categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "# print out summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "***\n",
    "Before training, we define the batch size and number of epochs. Since we are using the ImageDataGenerator to generate training data, we choose the **fit_generator** function to start fitting process. Other fitting methods are **fit** (feed in all training data at a time) and **train_on_batch** (perform single gradient descent on one batch). \n",
    "\n",
    "The information about the training process is returned as a **history** object. We can inspect the information in the history object and diagnose the training afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "batch_size = 1024\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                              steps_per_epoch = X_train.shape[0]/batch_size, \n",
    "                              epochs = epochs,\n",
    "                              validation_data = (X_val, y_val), \n",
    "                              shuffle = True, \n",
    "                              workers = -1, \n",
    "                              verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training process\n",
    "***\n",
    "In history.history, we can find the loss and accuracy on train/validation data for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,4))\n",
    "ax[0].plot((history.history['loss']), 'r', label='train')\n",
    "ax[0].plot((history.history['val_loss']), 'b' ,label='val')\n",
    "ax[0].set(xlabel = 'Epoch', ylabel = 'Loss', title = 'Training vs. Validation Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot((history.history['acc']), 'r', label='train')\n",
    "ax[1].plot((history.history['val_acc']), 'b' ,label='val')\n",
    "ax[1].set(xlabel = 'Epoch', ylabel = 'Accuracy', title = 'Training vs. Validation Accuracy')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "***\n",
    "Now we use the **evaluate** function to see how our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('loss = ', result[0], '\\naccuracy =', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use **predict** function to make prediction. It returns a vector of probability distribution over the 4 classes, so we use **np.argmax** to extract the predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis = 1)\n",
    "print('Average accuracy =', accuracy_score(np.argmax(y_test, axis = 1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "***\n",
    "For further inspect the model performance on each class, we typically plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use plotting function of confusion matrix from sklearn\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the normalized confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test,axis = 1),y_pred)\n",
    "plot_confusion_matrix(cm, class_name, normalize=False)\n",
    "\n",
    "print('\\nSensitivity :',np.diag(cm)/np.sum(cm,axis = 1))\n",
    "print('Positive predictive value :',np.diag(cm)/np.sum(cm,axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
